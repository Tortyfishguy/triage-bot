import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

# üîπ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÄ‡∏Å‡πá‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•
MODEL_NAME = "deepseek-ai/deepseek-llm-7b-chat"
MODEL_DIR = "./deepseek_model"

# üîπ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÄ‡∏Å‡πá‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏≤‡∏Å‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
if not os.path.exists(MODEL_DIR):
    os.makedirs(MODEL_DIR)

# üîπ ‡πÇ‡∏´‡∏•‡∏î Tokenizer ‡πÅ‡∏•‡∏∞ Model ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ offload ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î RAM
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=MODEL_DIR)

# üîπ ‡πÉ‡∏ä‡πâ quantization 4-bit ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î RAM ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16
)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16,
    device_map="auto",
    offload_folder=MODEL_DIR,  # ‡πÉ‡∏ä‡πâ offload ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ RAM
    quantization_config=quantization_config
)

def classify_esi(symptoms):
    """
    üîπ ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏£‡∏∞‡∏î‡∏±‡∏ö ESI ‡∏à‡∏≤‡∏Å‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ DeepSeek AI
    """
    prompt = (
        "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏û‡∏ó‡∏¢‡πå‡πÄ‡∏ß‡∏ä‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏â‡∏∏‡∏Å‡πÄ‡∏â‡∏¥‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏ï‡∏≤‡∏° Emergency Severity Index (ESI)\n"
        "ESI ‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô 5 ‡∏£‡∏∞‡∏î‡∏±‡∏ö:\n"
        "- ESI 1: ‡∏†‡∏≤‡∏ß‡∏∞‡∏â‡∏∏‡∏Å‡πÄ‡∏â‡∏¥‡∏ô‡∏ß‡∏¥‡∏Å‡∏§‡∏ï‡∏¥ ‡πÄ‡∏ä‡πà‡∏ô ‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏´‡∏¢‡∏∏‡∏î‡πÄ‡∏ï‡πâ‡∏ô, ‡∏´‡∏¢‡∏∏‡∏î‡∏´‡∏≤‡∏¢‡πÉ‡∏à, ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡∏±‡∏ô‡∏ï‡∏Å‡∏ß‡∏¥‡∏Å‡∏§‡∏ï‡∏¥\n"
        "- ESI 2: ‡∏†‡∏≤‡∏ß‡∏∞‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏™‡∏π‡∏á ‡πÄ‡∏ä‡πà‡∏ô ‡∏™‡∏±‡∏ö‡∏™‡∏ô, ‡∏´‡∏°‡∏î‡∏™‡∏ï‡∏¥, ‡∏´‡∏≠‡∏ö‡πÄ‡∏´‡∏ô‡∏∑‡πà‡∏≠‡∏¢‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á\n"
        "- ESI 3: ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏´‡∏•‡∏≤‡∏¢‡∏≠‡∏¢‡πà‡∏≤‡∏á ‡πÄ‡∏ä‡πà‡∏ô ‡∏ï‡∏£‡∏ß‡∏à‡πÄ‡∏•‡∏∑‡∏≠‡∏î, X-ray, CT Scan\n"
        "- ESI 4: ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏¢‡πá‡∏ö‡πÅ‡∏ú‡∏•, ‡∏ó‡∏≥‡πÅ‡∏ú‡∏•\n"
        "- ESI 5: ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå ‡πÄ‡∏ä‡πà‡∏ô ‡πÑ‡∏Ç‡πâ‡∏´‡∏ß‡∏±‡∏î‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢, ‡∏ú‡∏∑‡πà‡∏ô‡∏Ñ‡∏±‡∏ô\n\n"
        "‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢: "
        f"{symptoms}\n\n"
        "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö ESI ‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç 1-5 ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏´‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤ '‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÑ‡∏î‡πâ'"
    )

    device = "cuda" if torch.cuda.is_available() else "cpu"
    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=10)

    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()

    # üîπ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö
    for esi in ["1", "2", "3", "4", "5"]:
        if esi in response:
            return int(esi)

    return "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÑ‡∏î‡πâ"  # ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÑ‡∏î‡πâ
